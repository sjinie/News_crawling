{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in /opt/homebrew/lib/python3.13/site-packages (2.2.3)\n",
            "Collecting newspaper3k\n",
            "  Downloading newspaper3k-0.2.8-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting nltk\n",
            "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting google-generativeai\n",
            "  Downloading google_generativeai-0.8.5-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting scikit-learn\n",
            "  Downloading scikit_learn-1.6.1-cp313-cp313-macosx_12_0_arm64.whl.metadata (31 kB)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /opt/homebrew/lib/python3.13/site-packages (from pandas) (2.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/homebrew/lib/python3.13/site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/lib/python3.13/site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /opt/homebrew/lib/python3.13/site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: beautifulsoup4>=4.4.1 in /opt/homebrew/lib/python3.13/site-packages (from newspaper3k) (4.13.3)\n",
            "Requirement already satisfied: Pillow>=3.3.0 in /opt/homebrew/lib/python3.13/site-packages (from newspaper3k) (11.0.0)\n",
            "Collecting PyYAML>=3.11 (from newspaper3k)\n",
            "  Downloading PyYAML-6.0.2-cp313-cp313-macosx_11_0_arm64.whl.metadata (2.1 kB)\n",
            "Collecting cssselect>=0.9.2 (from newspaper3k)\n",
            "  Downloading cssselect-1.3.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: lxml>=3.6.0 in /opt/homebrew/lib/python3.13/site-packages (from newspaper3k) (5.3.0)\n",
            "Requirement already satisfied: requests>=2.10.0 in /opt/homebrew/lib/python3.13/site-packages (from newspaper3k) (2.32.3)\n",
            "Requirement already satisfied: feedparser>=5.2.1 in /opt/homebrew/lib/python3.13/site-packages (from newspaper3k) (6.0.11)\n",
            "Collecting tldextract>=2.0.1 (from newspaper3k)\n",
            "  Downloading tldextract-5.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting feedfinder2>=0.0.4 (from newspaper3k)\n",
            "  Downloading feedfinder2-0.0.4.tar.gz (3.3 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting jieba3k>=0.35.1 (from newspaper3k)\n",
            "  Downloading jieba3k-0.35.1.zip (7.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting tinysegmenter==0.3 (from newspaper3k)\n",
            "  Downloading tinysegmenter-0.3.tar.gz (16 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting click (from nltk)\n",
            "  Downloading click-8.2.0-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting joblib (from nltk)\n",
            "  Downloading joblib-1.5.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting regex>=2021.8.3 (from nltk)\n",
            "  Downloading regex-2024.11.6-cp313-cp313-macosx_11_0_arm64.whl.metadata (40 kB)\n",
            "Collecting tqdm (from nltk)\n",
            "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
            "Collecting google-ai-generativelanguage==0.6.15 (from google-generativeai)\n",
            "  Downloading google_ai_generativelanguage-0.6.15-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting google-api-core (from google-generativeai)\n",
            "  Downloading google_api_core-2.24.2-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting google-api-python-client (from google-generativeai)\n",
            "  Downloading google_api_python_client-2.169.0-py3-none-any.whl.metadata (6.7 kB)\n",
            "Collecting google-auth>=2.15.0 (from google-generativeai)\n",
            "  Downloading google_auth-2.40.1-py2.py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting protobuf (from google-generativeai)\n",
            "  Downloading protobuf-6.31.0-cp39-abi3-macosx_10_9_universal2.whl.metadata (593 bytes)\n",
            "Collecting pydantic (from google-generativeai)\n",
            "  Downloading pydantic-2.11.4-py3-none-any.whl.metadata (66 kB)\n",
            "Requirement already satisfied: typing-extensions in /opt/homebrew/lib/python3.13/site-packages (from google-generativeai) (4.13.2)\n",
            "Collecting google-api-core (from google-generativeai)\n",
            "  Downloading google_api_core-2.25.0rc1-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting proto-plus<2.0.0dev,>=1.22.3 (from google-ai-generativelanguage==0.6.15->google-generativeai)\n",
            "  Downloading proto_plus-1.26.1-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting protobuf (from google-generativeai)\n",
            "  Downloading protobuf-5.29.4-cp38-abi3-macosx_10_9_universal2.whl.metadata (592 bytes)\n",
            "Collecting scipy>=1.6.0 (from scikit-learn)\n",
            "  Downloading scipy-1.15.3-cp313-cp313-macosx_14_0_arm64.whl.metadata (61 kB)\n",
            "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
            "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: soupsieve>1.2 in /opt/homebrew/lib/python3.13/site-packages (from beautifulsoup4>=4.4.1->newspaper3k) (2.6)\n",
            "Requirement already satisfied: six in /opt/homebrew/lib/python3.13/site-packages (from feedfinder2>=0.0.4->newspaper3k) (1.16.0)\n",
            "Requirement already satisfied: sgmllib3k in /opt/homebrew/lib/python3.13/site-packages (from feedparser>=5.2.1->newspaper3k) (1.0.0)\n",
            "Collecting googleapis-common-protos<2.0.0,>=1.56.2 (from google-api-core->google-generativeai)\n",
            "  Downloading googleapis_common_protos-1.70.0-py3-none-any.whl.metadata (9.3 kB)\n",
            "Collecting cachetools<6.0,>=2.0.0 (from google-auth>=2.15.0->google-generativeai)\n",
            "  Downloading cachetools-5.5.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting pyasn1-modules>=0.2.1 (from google-auth>=2.15.0->google-generativeai)\n",
            "  Downloading pyasn1_modules-0.4.2-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting rsa<5,>=3.1.4 (from google-auth>=2.15.0->google-generativeai)\n",
            "  Downloading rsa-4.9.1-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/lib/python3.13/site-packages (from requests>=2.10.0->newspaper3k) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/lib/python3.13/site-packages (from requests>=2.10.0->newspaper3k) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/lib/python3.13/site-packages (from requests>=2.10.0->newspaper3k) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.13/site-packages (from requests>=2.10.0->newspaper3k) (2025.1.31)\n",
            "Collecting requests-file>=1.4 (from tldextract>=2.0.1->newspaper3k)\n",
            "  Downloading requests_file-2.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /opt/homebrew/lib/python3.13/site-packages (from tldextract>=2.0.1->newspaper3k) (3.18.0)\n",
            "Collecting httplib2<1.0.0,>=0.19.0 (from google-api-python-client->google-generativeai)\n",
            "  Downloading httplib2-0.22.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting google-auth-httplib2<1.0.0,>=0.2.0 (from google-api-python-client->google-generativeai)\n",
            "  Downloading google_auth_httplib2-0.2.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting uritemplate<5,>=3.0.1 (from google-api-python-client->google-generativeai)\n",
            "  Downloading uritemplate-4.1.1-py2.py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting annotated-types>=0.6.0 (from pydantic->google-generativeai)\n",
            "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting pydantic-core==2.33.2 (from pydantic->google-generativeai)\n",
            "  Downloading pydantic_core-2.33.2-cp313-cp313-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
            "Collecting typing-inspection>=0.4.0 (from pydantic->google-generativeai)\n",
            "  Downloading typing_inspection-0.4.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting grpcio<2.0.0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai)\n",
            "  Downloading grpcio-1.71.0-cp313-cp313-macosx_10_14_universal2.whl.metadata (3.8 kB)\n",
            "Collecting grpcio-status<2.0.0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai)\n",
            "  Downloading grpcio_status-1.71.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /opt/homebrew/lib/python3.13/site-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.0)\n",
            "Collecting pyasn1<0.7.0,>=0.6.1 (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai)\n",
            "  Downloading pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
            "Downloading newspaper3k-0.2.8-py3-none-any.whl (211 kB)\n",
            "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_generativeai-0.8.5-py3-none-any.whl (155 kB)\n",
            "Downloading google_ai_generativelanguage-0.6.15-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scikit_learn-1.6.1-cp313-cp313-macosx_12_0_arm64.whl (11.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.1/11.1 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading cssselect-1.3.0-py3-none-any.whl (18 kB)\n",
            "Downloading google_api_core-2.25.0rc1-py3-none-any.whl (160 kB)\n",
            "Downloading google_auth-2.40.1-py2.py3-none-any.whl (216 kB)\n",
            "Downloading joblib-1.5.0-py3-none-any.whl (307 kB)\n",
            "Downloading protobuf-5.29.4-cp38-abi3-macosx_10_9_universal2.whl (417 kB)\n",
            "Downloading PyYAML-6.0.2-cp313-cp313-macosx_11_0_arm64.whl (171 kB)\n",
            "Downloading regex-2024.11.6-cp313-cp313-macosx_11_0_arm64.whl (284 kB)\n",
            "Downloading scipy-1.15.3-cp313-cp313-macosx_14_0_arm64.whl (22.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.4/22.4 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
            "Downloading tldextract-5.3.0-py3-none-any.whl (107 kB)\n",
            "Downloading click-8.2.0-py3-none-any.whl (102 kB)\n",
            "Downloading google_api_python_client-2.169.0-py3-none-any.whl (13.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading pydantic-2.11.4-py3-none-any.whl (443 kB)\n",
            "Downloading pydantic_core-2.33.2-cp313-cp313-macosx_11_0_arm64.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
            "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
            "Downloading cachetools-5.5.2-py3-none-any.whl (10 kB)\n",
            "Downloading google_auth_httplib2-0.2.0-py2.py3-none-any.whl (9.3 kB)\n",
            "Downloading googleapis_common_protos-1.70.0-py3-none-any.whl (294 kB)\n",
            "Downloading httplib2-0.22.0-py3-none-any.whl (96 kB)\n",
            "Downloading proto_plus-1.26.1-py3-none-any.whl (50 kB)\n",
            "Downloading pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\n",
            "Downloading requests_file-2.1.0-py2.py3-none-any.whl (4.2 kB)\n",
            "Downloading rsa-4.9.1-py3-none-any.whl (34 kB)\n",
            "Downloading typing_inspection-0.4.0-py3-none-any.whl (14 kB)\n",
            "Downloading uritemplate-4.1.1-py2.py3-none-any.whl (10 kB)\n",
            "Downloading grpcio-1.71.0-cp313-cp313-macosx_10_14_universal2.whl (11.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading grpcio_status-1.71.0-py3-none-any.whl (14 kB)\n",
            "Downloading pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
            "Building wheels for collected packages: tinysegmenter, feedfinder2, jieba3k\n",
            "  Building wheel for tinysegmenter (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for tinysegmenter: filename=tinysegmenter-0.3-py3-none-any.whl size=13634 sha256=320bb1cf3b7c7a5371d96e4ba670685beb55a9613f87faad1dc1e9e8063111d4\n",
            "  Stored in directory: /Users/sj/Library/Caches/pip/wheels/3e/4c/d7/e4fff9472f65368e1ad77d2059d8183769f7e2d91c09a819a4\n",
            "  Building wheel for feedfinder2 (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for feedfinder2: filename=feedfinder2-0.0.4-py3-none-any.whl size=3393 sha256=bc0f3af0c97dda6b475d6c55451c96a985f7ac4298675cbcc61b66277e395958\n",
            "  Stored in directory: /Users/sj/Library/Caches/pip/wheels/2a/eb/9d/1963eb54f43f53573eaa7a0264cf9c0bf38747abd4aa18eeb8\n",
            "  Building wheel for jieba3k (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for jieba3k: filename=jieba3k-0.35.1-py3-none-any.whl size=7398403 sha256=53dc2b067d89f2962910745e1ef54ae65248fdddf976755511c21b7aca6298ea\n",
            "  Stored in directory: /Users/sj/Library/Caches/pip/wheels/1a/e6/27/8c2a31442ca32b7353dce2f7d40dbe5aae0bcdc32a11b82444\n",
            "Successfully built tinysegmenter feedfinder2 jieba3k\n",
            "Installing collected packages: tinysegmenter, jieba3k, uritemplate, typing-inspection, tqdm, threadpoolctl, scipy, regex, PyYAML, pydantic-core, pyasn1, protobuf, joblib, httplib2, grpcio, cssselect, click, cachetools, annotated-types, scikit-learn, rsa, requests-file, pydantic, pyasn1-modules, proto-plus, nltk, googleapis-common-protos, feedfinder2, tldextract, grpcio-status, google-auth, newspaper3k, google-auth-httplib2, google-api-core, google-api-python-client, google-ai-generativelanguage, google-generativeai\n",
            "Successfully installed PyYAML-6.0.2 annotated-types-0.7.0 cachetools-5.5.2 click-8.2.0 cssselect-1.3.0 feedfinder2-0.0.4 google-ai-generativelanguage-0.6.15 google-api-core-2.25.0rc1 google-api-python-client-2.169.0 google-auth-2.40.1 google-auth-httplib2-0.2.0 google-generativeai-0.8.5 googleapis-common-protos-1.70.0 grpcio-1.71.0 grpcio-status-1.71.0 httplib2-0.22.0 jieba3k-0.35.1 joblib-1.5.0 newspaper3k-0.2.8 nltk-3.9.1 proto-plus-1.26.1 protobuf-5.29.4 pyasn1-0.6.1 pyasn1-modules-0.4.2 pydantic-2.11.4 pydantic-core-2.33.2 regex-2024.11.6 requests-file-2.1.0 rsa-4.9.1 scikit-learn-1.6.1 scipy-1.15.3 threadpoolctl-3.6.0 tinysegmenter-0.3 tldextract-5.3.0 tqdm-4.67.1 typing-inspection-0.4.0 uritemplate-4.1.1\n"
          ]
        }
      ],
      "source": [
        "!pip3 install pandas newspaper3k nltk google-generativeai scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mR-8PPzLnSFb",
        "outputId": "9d8a4527-9e85-441b-abf1-56f998f28aa1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "'./data/' 폴더에 'dailycoffeenews_250503.csv', 'worldcoffeeportal_250503.csv', 'coffee_c_price.csv' 파일이 있는지 확인해주세요.\n",
            "--- 1단계: 뉴스 수집 및 LLM 라벨링 ---\n",
            "크롤링된 뉴스 파일 로드 중...\n",
            "  Daily Coffee News 파일 로드: data/dailycoffeenews_250503.csv\n",
            "  World Coffee Portal 파일 로드: data/worldcoffeeportal_250503.csv\n",
            "필터링 및 병합 후 LLM 라벨링 대상 기사 수: 324\n",
            "324개 기사 LLM 라벨링 시작...\n",
            "LLM 라벨링 완료. 유효 라벨 324개.\n",
            "\n",
            "--- 2단계: 가격 데이터 통합 ---\n",
            "최종 데이터 저장 완료: crawled_llm_labeled_news_with_prices_2025-01-01_to_2025-03-30.csv\n",
            "\n",
            "결과 샘플:\n",
            "         date                                              title  \\\n",
            "0  2025-01-01  Study Shows Links Between Coffee Drinking and ...   \n",
            "1  2025-01-01  Design Details: Atomic Coffee Roasters Spreadi...   \n",
            "2  2025-01-02  Three Questions with Filmmaker and Third Space...   \n",
            "3  2025-01-02  Iowa’s Euphoria Coffee Finds a Happy Home in C...   \n",
            "4  2025-01-02  Luckin Coffee launches in Hong Kong with five ...   \n",
            "\n",
            "                                                 url             source  \\\n",
            "0  https://dailycoffeenews.com/2025/01/01/study-s...    dailycoffeenews   \n",
            "1  https://dailycoffeenews.com/2025/01/01/design-...    dailycoffeenews   \n",
            "2  https://dailycoffeenews.com/2025/01/02/three-q...    dailycoffeenews   \n",
            "3  https://dailycoffeenews.com/2025/01/02/iowas-e...    dailycoffeenews   \n",
            "4  https://www.worldcoffeeportal.com/news/luckin-...  worldcoffeeportal   \n",
            "\n",
            "  llm_pseudo_label  close_price  next_day_close_price  \\\n",
            "0          neutral          NaN                   NaN   \n",
            "1          neutral          NaN                   NaN   \n",
            "2          neutral   326.850006            318.649994   \n",
            "3          neutral   326.850006            318.649994   \n",
            "4          neutral   326.850006            318.649994   \n",
            "\n",
            "   actual_price_return_pct actual_price_label  \\\n",
            "0                      NaN                NaN   \n",
            "1                      NaN                NaN   \n",
            "2                  -2.5088                 중립   \n",
            "3                  -2.5088                 중립   \n",
            "4                  -2.5088                 중립   \n",
            "\n",
            "                                        text_for_llm  \n",
            "0  title: study shows links between coffee drinki...  \n",
            "1  title: design details: atomic coffee roasters ...  \n",
            "2  title: three questions with filmmaker and thir...  \n",
            "3  title: iowa’s euphoria coffee finds a happy ho...  \n",
            "4  title: luckin coffee launches in hong kong wit...  \n",
            "\n",
            "LLM 라벨 vs 실제 가격 변동 (비교 가능 행):\n",
            "actual_price_label  상승   중립  하락\n",
            "llm_pseudo_label               \n",
            "neutral             16  267  15\n",
            "price-down           0    3   0\n",
            "price-up             0    8   0\n",
            "\n",
            "스크립트 실행 완료.\n"
          ]
        }
      ],
      "source": [
        "import datetime\n",
        "import re\n",
        "import time\n",
        "from pathlib import Path\n",
        "import nltk\n",
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "import google.generativeai as genai\n",
        "from google.colab import userdata # Colab API 세팅 필요\n",
        "\n",
        "# --- 기본 설정 ---\n",
        "try:\n",
        "    GEMINI_API_KEY = userdata.get(\"GOOGLE_API_KEY\")\n",
        "    if not GEMINI_API_KEY:\n",
        "        print(\"WARN: 'GOOGLE_API_KEY' not found in Colab userdata or is empty.\")\n",
        "        GEMINI_API_KEY = None\n",
        "except Exception as e:\n",
        "    print(f\"WARN: Error accessing Colab userdata for 'GOOGLE_API_KEY': {e}\")\n",
        "    GEMINI_API_KEY = None\n",
        "\n",
        "nltk.download(\"stopwords\", quiet=True, raise_on_error=False)\n",
        "STOP_WORDS = set(stopwords.words(\"english\"))\n",
        "\n",
        "# 뉴스 파일 로드 및 날짜 범위 설정\n",
        "NEWS_START_DATE_STR = \"2025-01-01\" # LLM 라벨링 대상 시작일\n",
        "NEWS_END_DATE_STR = \"2025-03-30\"   # LLM 라벨링 대상 종료일\n",
        "NEWS_START_DATE = datetime.datetime.strptime(NEWS_START_DATE_STR, \"%Y-%m-%d\").date()\n",
        "NEWS_END_DATE = datetime.datetime.strptime(NEWS_END_DATE_STR, \"%Y-%m-%d\").date()\n",
        "\n",
        "LLM_MODEL_NAME = \"gemini-2.0-flash\"\n",
        "LLM_REQUEST_DELAY = 2\n",
        "LLM_PRICE_LABELS = [\"price-down\", \"neutral\", \"price-up\"]\n",
        "\n",
        "PRICE_FILE_PATH = \"./data/coffee_c_price.csv\"\n",
        "PRICE_CHANGE_THRESHOLD_PERCENT = 4.0\n",
        "\n",
        "DATA_DIR = \"./data/\"\n",
        "DAILYCOFFEENEWS_CSV = Path(DATA_DIR) / \"dailycoffeenews_250503.csv\"\n",
        "WORLDCOFFEEPORTAL_CSV = Path(DATA_DIR) / \"worldcoffeeportal_250503.csv\"\n",
        "\n",
        "\n",
        "# --- 함수 정의 ---\n",
        "def fix_encoding(text):\n",
        "    if not isinstance(text, str): return text\n",
        "    try:\n",
        "        return text.encode(\"raw_unicode_escape\").decode(\"utf-8\", \"ignore\").encode(\"latin1\").decode(\"utf-8\", \"ignore\")\n",
        "    except: return text\n",
        "\n",
        "def preprocess_text_for_llm(title, body=None):\n",
        "    full_text = f\"Title: {title or ''}\".strip().lower()\n",
        "    if body:\n",
        "        full_text += f\"\\n\\nBody: {body or ''}\"\n",
        "    return re.sub(r\"http\\S+\", \"[URL REMOVED]\", full_text).strip()\n",
        "\n",
        "\n",
        "def load_and_combine_crawled_news_from_specific_files(\n",
        "    daily_news_file: Path,\n",
        "    portal_news_file: Path,\n",
        "    start_date_limit: datetime.date,\n",
        "    end_date_limit: datetime.date\n",
        ") -> pd.DataFrame:\n",
        "    print(\"크롤링된 뉴스 파일 로드 중...\")\n",
        "\n",
        "    df_daily = pd.DataFrame()\n",
        "    df_portal = pd.DataFrame()\n",
        "\n",
        "    if daily_news_file.exists():\n",
        "        print(f\"  Daily Coffee News 파일 로드: {daily_news_file}\")\n",
        "        try:\n",
        "            df_daily = pd.read_csv(daily_news_file)\n",
        "            df_daily['source'] = 'dailycoffeenews'\n",
        "        except Exception as e:\n",
        "            print(f\"    오류: {daily_news_file} 로드 실패 - {e}\")\n",
        "    else:\n",
        "        print(f\"  경고: Daily Coffee News 파일 없음 - {daily_news_file}\")\n",
        "\n",
        "    if portal_news_file.exists():\n",
        "        print(f\"  World Coffee Portal 파일 로드: {portal_news_file}\")\n",
        "        try:\n",
        "            df_portal = pd.read_csv(portal_news_file)\n",
        "            df_portal['source'] = 'worldcoffeeportal'\n",
        "        except Exception as e:\n",
        "            print(f\"    오류: {portal_news_file} 로드 실패 - {e}\")\n",
        "    else:\n",
        "        print(f\"  경고: World Coffee Portal 파일 없음 - {portal_news_file}\")\n",
        "\n",
        "    if df_daily.empty and df_portal.empty:\n",
        "        print(\"로드할 뉴스 파일이 없습니다.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    combined_df = pd.concat([df_daily, df_portal], ignore_index=True)\n",
        "    if 'date' not in combined_df.columns or 'title' not in combined_df.columns:\n",
        "        print(\"오류: CSV 파일에 'date' 또는 'title' 컬럼이 없습니다.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    combined_df['date'] = pd.to_datetime(combined_df['date'], errors='coerce').dt.date\n",
        "    combined_df.dropna(subset=['date', 'title'], inplace=True)\n",
        "\n",
        "    combined_df = combined_df[(combined_df['date'] >= start_date_limit) & (combined_df['date'] <= end_date_limit)]\n",
        "\n",
        "    if 'url' in combined_df.columns:\n",
        "        combined_df.drop_duplicates(subset=['url'], keep='first', inplace=True)\n",
        "    else:\n",
        "        combined_df.drop_duplicates(subset=['date', 'title'], keep='first', inplace=True)\n",
        "\n",
        "    combined_df.sort_values(by=\"date\", inplace=True)\n",
        "\n",
        "    combined_df[\"text_for_llm\"] = combined_df[\"title\"].apply(lambda x: preprocess_text_for_llm(x))\n",
        "    combined_df = combined_df[combined_df[\"text_for_llm\"] != \"\"].reset_index(drop=True)\n",
        "\n",
        "    print(f\"필터링 및 병합 후 LLM 라벨링 대상 기사 수: {len(combined_df)}\")\n",
        "    return combined_df[['date', 'title', 'url', 'text_for_llm', 'source']]\n",
        "\n",
        "\n",
        "def get_llm_pseudo_label_gemini(article_text, llm_model_client):\n",
        "    if not llm_model_client:\n",
        "        import random\n",
        "        return random.choice(LLM_PRICE_LABELS)\n",
        "    \n",
        "    prompt = f\"\"\"Analyze the coffee market sentiment of the following news article.\n",
        "Label it as 'price-up', 'price-down', or 'neutral'. Return ONLY the label.\n",
        "\n",
        "Examples:\n",
        "Article: \"Title: Brazil Frosts Decimate Coffee Crop Body: Severe frost damages arabica output.\"\n",
        "Label: price-up\n",
        "\n",
        "Article: \"Title: ICO Reports Record Global Coffee Exports Body: Strong harvest in Vietnam boosts exports.\"\n",
        "Label: price-down\n",
        "\n",
        "Article: \"Title: Nestlé Launches New Coffee Line Body: Nestlé expands premium coffee offerings.\"\n",
        "Label: neutral\n",
        "---\n",
        "Article: \"{article_text}\"\n",
        "Label:\"\"\"\n",
        "    try:\n",
        "        safety_settings=[{\"category\": c,\"threshold\": \"BLOCK_NONE\"} for c in [\"HARM_CATEGORY_HARASSMENT\",\"HARM_CATEGORY_HATE_SPEECH\",\"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\"HARM_CATEGORY_DANGEROUS_CONTENT\"]]\n",
        "        response = llm_model_client.generate_content(prompt, safety_settings=safety_settings)\n",
        "        llm_label = \"\"\n",
        "        if response.parts:\n",
        "             llm_label = \"\".join(part.text for part in response.parts if hasattr(part, 'text')).strip().lower()\n",
        "        elif hasattr(response, 'text') and response.text:\n",
        "             llm_label = response.text.strip().lower()\n",
        "        else:\n",
        "            if response.prompt_feedback and response.prompt_feedback.block_reason:\n",
        "                print(f\"    경고: Gemini가 프롬프트를 차단했습니다. 이유: {response.prompt_feedback.block_reason}. neutral로 기본 설정.\")\n",
        "            else:\n",
        "                print(f\"    경고: Gemini가 비어 있거나 예기치 않은 응답 구조를 반환했습니다. neutral로 기본 설정.\")\n",
        "            return \"neutral\"\n",
        "        return llm_label if llm_label in LLM_PRICE_LABELS else \"neutral\"\n",
        "    except Exception as e:\n",
        "        return \"neutral\"\n",
        "\n",
        "def generate_pseudo_labels_with_gemini(df):\n",
        "    print(f\"{len(df)}개 기사 LLM 라벨링 시작...\")\n",
        "    llm_model_client = None\n",
        "    if GEMINI_API_KEY:\n",
        "        try:\n",
        "            genai.configure(api_key=GEMINI_API_KEY)\n",
        "            llm_model_client = genai.GenerativeModel(LLM_MODEL_NAME)\n",
        "        except Exception as e:\n",
        "            print(f\"Gemini 모델 초기화 오류: {e}. 임의 라벨링 사용.\")\n",
        "    else:\n",
        "        print(\"Gemini API 키 없음. 임의 라벨링 사용.\")\n",
        "\n",
        "    labels = []\n",
        "    for i, row in df.iterrows():\n",
        "        pseudo_label = get_llm_pseudo_label_gemini(row['text_for_llm'][:25000], llm_model_client) # 텍스트 길이 제한\n",
        "        labels.append(pseudo_label)\n",
        "        if llm_model_client: time.sleep(LLM_REQUEST_DELAY)\n",
        "    df[\"llm_pseudo_label\"] = labels\n",
        "    df = df[df[\"llm_pseudo_label\"].isin(LLM_PRICE_LABELS)]\n",
        "    print(f\"LLM 라벨링 완료. 유효 라벨 {len(df)}개.\")\n",
        "    return df\n",
        "\n",
        "def load_and_prepare_price_data(price_file_path, threshold):\n",
        "    if not Path(price_file_path).exists():\n",
        "        print(f\"가격 파일 없음: {price_file_path}\")\n",
        "        return pd.DataFrame()\n",
        "    price_df = pd.read_csv(price_file_path)\n",
        "    price_df.rename(columns={\"Date\": \"date\", \"Coffee_Price\": \"close_price\"}, inplace=True)\n",
        "    price_df['date'] = pd.to_datetime(price_df['date'], errors='coerce').dt.date\n",
        "    price_df['close_price'] = pd.to_numeric(price_df['close_price'], errors='coerce')\n",
        "    price_df.dropna(subset=['date', 'close_price'], inplace=True)\n",
        "    price_df.sort_values(by=\"date\", inplace=True)\n",
        "    price_df[\"next_day_close_price\"] = price_df[\"close_price\"].shift(-1)\n",
        "    price_df[\"actual_price_return_pct\"] = ((price_df[\"next_day_close_price\"] - price_df[\"close_price\"]) / price_df[\"close_price\"]) * 100\n",
        "    def classify_actual_movement(pct_change):\n",
        "        if pd.isna(pct_change): return \"정보없음\"\n",
        "        if pct_change >= threshold: return \"상승\"\n",
        "        elif pct_change <= -threshold: return \"하락\"\n",
        "        else: return \"중립\"\n",
        "    price_df[\"actual_price_label\"] = price_df[\"actual_price_return_pct\"].apply(classify_actual_movement)\n",
        "    return price_df[[\"date\", \"close_price\", \"next_day_close_price\", \"actual_price_return_pct\", \"actual_price_label\"]]\n",
        "\n",
        "# --- 메인 실행 ---\n",
        "if __name__ == \"__main__\":\n",
        "    Path(DATA_DIR).mkdir(parents=True, exist_ok=True)\n",
        "    print(f\"'{DATA_DIR}' 폴더에 '{DAILYCOFFEENEWS_CSV.name}', '{WORLDCOFFEEPORTAL_CSV.name}', '{Path(PRICE_FILE_PATH).name}' 파일이 있는지 확인해주세요.\")\n",
        "\n",
        "    print(\"--- 1단계: 뉴스 수집 및 LLM 라벨링 ---\")\n",
        "    news_to_label_df = load_and_combine_crawled_news_from_specific_files(\n",
        "        daily_news_file=DAILYCOFFEENEWS_CSV,\n",
        "        portal_news_file=WORLDCOFFEEPORTAL_CSV,\n",
        "        start_date_limit=NEWS_START_DATE,\n",
        "        end_date_limit=NEWS_END_DATE\n",
        "    )\n",
        "\n",
        "    if news_to_label_df.empty:\n",
        "        print(\"지정된 날짜 범위의 수집된 뉴스가 없습니다. 종료.\")\n",
        "    else:\n",
        "        pseudo_labeled_news_df = generate_pseudo_labels_with_gemini(news_to_label_df.copy())\n",
        "        if pseudo_labeled_news_df.empty:\n",
        "            print(\"LLM 라벨링된 뉴스 없음. 종료.\")\n",
        "        else:\n",
        "            print(\"\\n--- 2단계: 가격 데이터 통합 ---\")\n",
        "            price_data_df = load_and_prepare_price_data(PRICE_FILE_PATH, PRICE_CHANGE_THRESHOLD_PERCENT)\n",
        "            if price_data_df.empty:\n",
        "                print(\"가격 데이터 로드 실패. LLM 라벨링된 뉴스만 저장합니다.\")\n",
        "                output_filename = f\"crawled_llm_labeled_news_{NEWS_START_DATE_STR}_to_{NEWS_END_DATE_STR}.csv\"\n",
        "                pseudo_labeled_news_df.to_csv(output_filename, index=False, encoding=\"utf-8-sig\")\n",
        "                print(f\"저장 완료: {output_filename}\")\n",
        "            else:\n",
        "                final_df = pd.merge(pseudo_labeled_news_df, price_data_df, on=\"date\", how=\"left\")\n",
        "                output_filename = f\"crawled_llm_labeled_news_with_prices_{NEWS_START_DATE_STR}_to_{NEWS_END_DATE_STR}.csv\"\n",
        "                output_columns = [\"date\", \"title\", \"url\", \"source\", \"llm_pseudo_label\",\n",
        "                                  \"close_price\", \"next_day_close_price\", \"actual_price_return_pct\", \"actual_price_label\",\n",
        "                                  \"text_for_llm\"]\n",
        "                final_df_output = final_df[[col for col in output_columns if col in final_df.columns]]\n",
        "                final_df_output.to_csv(output_filename, index=False, encoding=\"utf-8-sig\")\n",
        "                print(f\"최종 데이터 저장 완료: {output_filename}\")\n",
        "                print(\"\\n결과 샘플:\")\n",
        "                print(final_df_output.head())\n",
        "\n",
        "                comparable_df = final_df_output.dropna(subset=['actual_price_label', 'llm_pseudo_label'])\n",
        "                if not comparable_df.empty and comparable_df['actual_price_label'].nunique() > 0 and comparable_df['llm_pseudo_label'].nunique() > 0 :\n",
        "                    print(\"\\nLLM 라벨 vs 실제 가격 변동 (비교 가능 행):\")\n",
        "                    try:\n",
        "                        print(pd.crosstab(comparable_df['llm_pseudo_label'], comparable_df['actual_price_label']))\n",
        "                    except:\n",
        "                        print(\"크로스탭 생성 중 문제 발생.\")\n",
        "    print(\"\\n스크립트 실행 완료.\")\n",
        "    if not GEMINI_API_KEY:\n",
        "        print(\"주의: Gemini API 키가 설정되지 않아 임의 라벨이 사용되었을 수 있습니다.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
